{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Q2-1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6MlHCEAdZPy1","colab_type":"text"},"source":["**Initial step:** Please try to put the extracted CamVid folder in your Google Drive!\n","So now you could mount your data to this ipynb!"]},{"cell_type":"code","metadata":{"id":"UGAXY5HMYjzL","colab_type":"code","outputId":"f91d3efe-3502-4597-c9e4-0311d16c9322","executionInfo":{"status":"ok","timestamp":1577348824942,"user_tz":-480,"elapsed":2344,"user":{"displayName":"楊子儀","photoUrl":"","userId":"11501322789249152564"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wTBYCMcpZoA8","colab_type":"code","outputId":"0ae4b49a-a255-4f96-fce8-3c1de2eabbeb","executionInfo":{"status":"ok","timestamp":1577348833505,"user_tz":-480,"elapsed":10886,"user":{"displayName":"楊子儀","photoUrl":"","userId":"11501322789249152564"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["# if you mount Google drive correctly, the following commands should be able to executed correctly\n","!ls /content/drive/\n","%cd \"/content/drive/My Drive\"\n","%cd \"CamVid\"\n","\n","!ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["'My Drive'  'Shared drives'\n","/content/drive/My Drive\n","/content/drive/My Drive/CamVid\n","result_comparision  train  trainannot  train.csv  val  valannot  val.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pfWmPZKyZ8gA","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import utils\n","import torchvision\n","from torchvision import models\n","from torchvision.models.vgg import VGG\n","import random\n","\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import time\n","import sys\n","import os\n","from os import path\n","\n","from PIL import Image\n","import pandas as pd\n","from torchvision.models.vgg import VGG"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YMzL2KhcaI9u","colab_type":"code","outputId":"d4614fc7-e39b-4db7-cbb3-b939193788c9","executionInfo":{"status":"ok","timestamp":1577348833507,"user_tz":-480,"elapsed":10848,"user":{"displayName":"楊子儀","photoUrl":"","userId":"11501322789249152564"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["\n","root_dir   = \"/content/drive/My Drive/CamVid/\"\n","train_file = os.path.join(root_dir, \"train.csv\")\n","val_file   = os.path.join(root_dir, \"val.csv\")\n","\n","print(\"training csv exits:{}\".format(path.exists(train_file)))\n","print(\"validation csv exits:{}\".format(path.exists(val_file)))\n","\n","# the folder to save results for comparison\n","folder_to_save_validation_result = '/content/drive/My Drive/CamVid/result_comparision/' \n","\n","if os.path.isdir(folder_to_save_validation_result) == False:\n","    os.mkdir(folder_to_save_validation_result)\n","\n","\n","# the number of segmentation classes\n","num_class = 11 # 32 for original CamVid\n","means     = np.array([103.939, 116.779, 123.68]) / 255. # mean of three channels in the order of BGR\n","\n","h, w      = 256, 256\n","train_h = 256\n","train_w = 256\n","val_h = 256\n","val_w = 256\n","\n","## parameters for Solver-Adam in this example\n","batch_size = 6 #\n","epochs     = 20 # don't try to improve the performance by simply increasing the training epochs or iterations\n","lr         = 1e-4    # achieved besty results \n","step_size  = 100 # Won't work when epochs <=100\n","gamma      = 0.5 # \n","#\n","\n","## index for validation images\n","global_index = 0\n","\n","# pixel accuracy and mIOU list \n","pixel_acc_list = []\n","mIOU_list = []\n","\n","use_gpu = torch.cuda.is_available()\n","num_gpu = list(range(torch.cuda.device_count()))\n","\n","class CamVidDataset(Dataset):\n","\n","    def __init__(self, csv_file, phase, n_class=num_class, crop=True, flip_rate=0.5):\n","        self.data      = pd.read_csv(csv_file)\n","        self.means     = means\n","        self.n_class   = n_class\n","        self.flip_rate = flip_rate       \n","\n","        self.resize_h = h\n","        self.resize_w = w        \n","        \n","        if phase == 'train':\n","            self.new_h = train_h\n","            self.new_w = train_w\n","            self.crop = crop\n","        elif phase == 'val':\n","            self.flip_rate = 0.\n","            self.crop = False # False\n","            self.new_h = val_h\n","            self.new_w = val_w\n","\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_name   = self.data.iloc[idx, 0]                \n","        img_name = root_dir  + img_name                        \n","        img = Image.open(img_name).convert('RGB')  \n","\n","        label_name = self.data.iloc[idx, 1]        \n","        label_name = root_dir  + label_name                       \n","        label_image = Image.open(label_name)\n","        label = np.asarray(label_image)\n","\n","        # In training mode, the crop strategy is random-shift crop.\n","        # In validation model, it is center crop.\n","        if self.crop:            \n","            w, h = img.size\n","            A_x_offset = np.int32(np.random.randint(0, w - self.new_w + 1, 1))[0]\n","            A_y_offset = np.int32(np.random.randint(0, h - self.new_h + 1, 1))[0]\n","\n","            img = img.crop((A_x_offset, A_y_offset, A_x_offset + self.new_w, A_y_offset + self.new_h)) # left, top, right, bottom\n","            label_image = label_image.crop((A_x_offset, A_y_offset, A_x_offset + self.new_w, A_y_offset + self.new_h)) # left, top, right, bottom\n","        else:            \n","            w, h = img.size\n","            A_x_offset = int((w - self.new_w)/2)\n","            A_y_offset = int((h - self.new_h)/2)\n","            \n","            img = img.crop((A_x_offset, A_y_offset, A_x_offset + self.new_w, A_y_offset + self.new_h)) # left, top, right, bottom\n","            label_image = label_image.crop((A_x_offset, A_y_offset, A_x_offset + self.new_w, A_y_offset + self.new_h)) # left, top, right, bottom\n","\n","            label_image_h, label_image_w = label_image.size\n","\n","        # we could try to revise the values in label for reducing the number of segmentation classes\n","        label = np.array(label_image)              \n","\n","        if random.random() < self.flip_rate:\n","            img   = np.fliplr(img)\n","            label = np.fliplr(label)\n","        \n","        # reduce mean in terms of BGR\n","        img = np.transpose(img, (2, 0, 1)) / 255.\n","        img[0] -= self.means[0]\n","        img[1] -= self.means[1]\n","        img[2] -= self.means[2]\n","\n","        # convert to tensor\n","        img = torch.from_numpy(img.copy()).float()\n","        label = torch.from_numpy(label.copy()).long()\n","\n","        # create one-hot encoding\n","        h, w = label.size()\n","        target = torch.zeros(self.n_class, h, w)\n","        for c in range(self.n_class):\n","            target[c][label == c] = 1\n","\n","        sample = {'X': img, 'Y': target, 'l': label}\n","\n","        return sample\n","\n","\n","train_data = CamVidDataset(csv_file=train_file, phase='train')\n","train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=8)\n","\n","val_data = CamVidDataset(csv_file=val_file, phase='val', flip_rate=0)\n","val_loader = DataLoader(val_data, batch_size=1, num_workers=8)\n","\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["training csv exits:True\n","validation csv exits:True\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4h-a5moWhLhF","colab_type":"code","outputId":"68b5dc21-1e94-41ff-cf5e-bb5095fc6df1","executionInfo":{"status":"ok","timestamp":1577348840706,"user_tz":-480,"elapsed":18014,"user":{"displayName":"楊子儀","photoUrl":"","userId":"11501322789249152564"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# cropped version from https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n","cfg = {\n","    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","}\n","\n","ranges = {\n","    'vgg11': ((0, 3), (3, 6),  (6, 11),  (11, 16), (16, 21)),\n","    'vgg13': ((0, 5), (5, 10), (10, 15), (15, 20), (20, 25)),\n","    'vgg16': ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31)),\n","    'vgg19': ((0, 5), (5, 10), (10, 19), (19, 28), (28, 37))\n","}\n","\n","def make_layers(cfg, batch_norm=False):\n","    layers = []\n","    in_channels = 3\n","    for v in cfg:\n","        if v == 'M':\n","            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","        else:\n","            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n","            if batch_norm:\n","                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n","            else:\n","                layers += [conv2d, nn.ReLU(inplace=True)]\n","            in_channels = v\n","    return nn.Sequential(*layers)\n","\n","class VGGNet(VGG):\n","    def __init__(self, pretrained=True, model='vgg16', requires_grad=True, remove_fc=True, show_params=False):\n","        super().__init__(make_layers(cfg[model]))\n","        self.ranges = ranges[model]\n","\n","        if pretrained:            \n","            exec(\"self.load_state_dict(models.%s(pretrained=True).state_dict())\" % model)\n","\n","        if not requires_grad:\n","            for param in super().parameters():\n","                param.requires_grad = False\n","\n","        if remove_fc:  # delete redundant fully-connected layer params, can save memory\n","            del self.classifier\n","\n","        if show_params:\n","            for name, param in self.named_parameters():\n","                print(name, param.size())\n","\n","    def forward(self, x):\n","        output = {}\n","\n","        # get the output of each maxpooling layer (5 maxpool in VGG net)\n","        for idx in range(len(self.ranges)):\n","            for layer in range(self.ranges[idx][0], self.ranges[idx][1]):\n","                x = self.features[layer](x)\n","            output[\"x%d\"%(idx+1)] = x\n","\n","        return output\n","\n","class FCN8s(nn.Module):\n","    #Ref: https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1\n","    #The layer description is accordance with the above fiture instead of the original paper. Alex 2019/12/03 \n","    def __init__(self, pretrained_net, n_class):\n","        super().__init__()\n","        self.n_class = n_class\n","        self.pretrained_net = pretrained_net\n","        self.relu    = nn.ReLU(inplace=True)\n","        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn1     = nn.BatchNorm2d(512)\n","        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn2     = nn.BatchNorm2d(256)\n","        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn3     = nn.BatchNorm2d(128)\n","        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn4     = nn.BatchNorm2d(64)\n","        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n","        self.bn5     = nn.BatchNorm2d(32)\n","        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n","\n","    def forward(self, x):\n","        output = self.pretrained_net(x)\n","        # print(output)\n","\n","        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n","        x4 = output['x4']  # size=(N, 512, x.H/16, x.W/16)\n","        x3 = output['x3']  # size=(N, 256, x.H/8,  x.W/8)\n","\n","        # ## FCN8\n","        # score = self.relu(self.deconv1(x5))               # size=(N, 512, x.H/16, x.W/16)\n","        # score = self.bn1(score + x4)                      # element-wise add, size=(N, 512, x.H/16, x.W/16)                      \n","        # score = self.relu(self.deconv2(score))            # size=(N, 256, x.H/8, x.W/8)\n","        # score = self.bn2(score + x3)                      # element-wise add, size=(N, 256, x.H/8, x.W/8)           \n","        # score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n","        # score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n","        # score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n","        # score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n","\n","        ## FCN32\n","        score = self.relu(self.deconv1(x5))               # size=(N, 512, x.H/16, x.W/16)\n","        score = self.bn1(score)                           # element-wise add, size=(N, 512, x.H/16, x.W/16)                      \n","        score = self.relu(self.deconv2(score))            # size=(N, 256, x.H/8, x.W/8)\n","        score = self.bn2(score)                           # element-wise add, size=(N, 256, x.H/8, x.W/8)\n","        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n","        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n","        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n","        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n","\n","        return score  # size=(N, n_class, x.H/1, x.W/1)\n","\n","# load pretrained weights and define FCN8s\n","vgg_model = VGGNet(requires_grad=True, remove_fc=True)\n","fcn_model = FCN8s(pretrained_net=vgg_model, n_class=num_class)\n","\n","ts = time.time()\n","vgg_model = vgg_model.cuda()\n","fcn_model = fcn_model.cuda()\n","fcn_model = nn.DataParallel(fcn_model, device_ids=num_gpu)\n","print(\"Finish cuda loading, time elapsed {}\".format(time.time() - ts))\n","\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.Adam(fcn_model.parameters(), lr=lr)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)  "],"execution_count":5,"outputs":[{"output_type":"stream","text":["Finish cuda loading, time elapsed 2.0161938667297363\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lUjPeffKbm36","colab_type":"code","colab":{}},"source":["def train():\n","    for epoch in range(epochs):\n","        scheduler.step()\n","\n","        ts = time.time()\n","        for iter, batch in enumerate(train_loader):\n","            optimizer.zero_grad()\n","\n","            if use_gpu:\n","                inputs = Variable(batch['X'].cuda())\n","                labels = Variable(batch['Y'].cuda())\n","            else:\n","                inputs, labels = Variable(batch['X']), Variable(batch['Y'])\n","\n","            outputs = fcn_model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            if iter % 10 == 0:\n","                print(\"epoch{}, iter{}, loss: {}\".format(epoch, iter, loss.data.item()))\n","        \n","        print(\"Finish epoch {}, time elapsed {}\".format(epoch, time.time() - ts))\n","        \n","\n","        val(epoch)\n","        \n","    highest_pixel_acc = max(pixel_acc_list)\n","    highest_mIOU = max(mIOU_list)        \n","    \n","    highest_pixel_acc_epoch = pixel_acc_list.index(highest_pixel_acc)\n","    highest_mIOU_epoch = mIOU_list.index(highest_mIOU)\n","    \n","    print(\"The highest mIOU is {} and is achieved at epoch-{}\".format(highest_mIOU, highest_mIOU_epoch))\n","    print(\"The highest pixel accuracy  is {} and is achieved at epoch-{}\".format(highest_pixel_acc, highest_pixel_acc_epoch))\n","    \n","\n","\n","\n","\n","def save_result_comparison(input_np, output_np):\n","    means     = np.array([103.939, 116.779, 123.68]) / 255.\n","    \n","    global global_index\n","    \n","    original_im_RGB = np.zeros((256,256,3))    \n","    original_im_RGB[:,:,0] = input_np[0,0,:,:]    \n","    original_im_RGB[:,:,1] = input_np[0,1,:,:]\n","    original_im_RGB[:,:,2] = input_np[0,2,:,:]\n","        \n","    original_im_RGB[:,:,0] = original_im_RGB[:,:,0] + means[0]\n","    original_im_RGB[:,:,1] = original_im_RGB[:,:,1] + means[1]\n","    original_im_RGB[:,:,2] = original_im_RGB[:,:,2] + means[2]\n","        \n","    original_im_RGB[:,:,0] = original_im_RGB[:,:,0]*255.0\n","    original_im_RGB[:,:,1] = original_im_RGB[:,:,1]*255.0\n","    original_im_RGB[:,:,2] = original_im_RGB[:,:,2]*255.0\n","    \n","    im_seg_RGB = np.zeros((256,256,3))\n","\n","    # the following version is designed for 11-class version and could still work if the number of classes is fewer.\n","    for i in range(256):\n","        for j in range(256):\n","            if output_np[i,j] == 0:\n","                im_seg_RGB[i,j,:] = [128, 128, 128]\n","            elif output_np[i,j] == 1:  \n","                im_seg_RGB[i,j,:] = [128, 0, 0]\n","            elif output_np[i,j] == 2:  \n","                im_seg_RGB[i,j,:] = [192, 192, 128]    \n","            elif output_np[i,j] == 3:  \n","                im_seg_RGB[i,j,:] = [128, 64, 128]    \n","            elif output_np[i,j] == 4:  \n","                im_seg_RGB[i,j,:] = [0, 0, 192]    \n","            elif output_np[i,j] == 5:  \n","                im_seg_RGB[i,j,:] = [128, 128, 0]    \n","            elif output_np[i,j] == 6:  \n","                im_seg_RGB[i,j,:] = [192, 128, 128]    \n","            elif output_np[i,j] == 7:  \n","                im_seg_RGB[i,j,:] = [64, 64, 128]    \n","            elif output_np[i,j] == 8:  \n","                im_seg_RGB[i,j,:] = [64, 0, 128]    \n","            elif output_np[i,j] == 9:  \n","                im_seg_RGB[i,j,:] = [64, 64, 0]    \n","            elif output_np[i,j] == 10:  \n","                im_seg_RGB[i,j,:] = [0, 128, 192]    \n","                    \n","    # horizontally stack original image and its corresponding segmentation results     \n","    hstack_image = np.hstack((original_im_RGB, im_seg_RGB))             \n","    new_im = Image.fromarray(np.uint8(hstack_image))\n","    \n","    file_name = folder_to_save_validation_result + str(global_index) + '.jpg'\n","        \n","    global_index = global_index + 1\n","        \n","    new_im.save(file_name)       "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E7cK6h9vkbf7","colab_type":"code","outputId":"365afc44-d791-4a23-ddc0-1267a4d13419","executionInfo":{"status":"ok","timestamp":1577349599098,"user_tz":-480,"elapsed":776350,"user":{"displayName":"楊子儀","photoUrl":"","userId":"11501322789249152564"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["def val(epoch):\n","    fcn_model.eval()\n","    total_ious = []\n","    pixel_accs = []\n","                    \n","    \n","    for iter, batch in enumerate(val_loader): ## batch is 1 in this case\n","        if use_gpu:\n","            inputs = Variable(batch['X'].cuda())\n","        else:\n","            inputs = Variable(batch['X'])        \n","\n","        output = fcn_model(inputs)                                \n","        \n","        # only save the 1st image for comparison\n","        if iter == 0:\n","            print('---------iter={}'.format(iter))\n","            # generate images\n","            images = output.data.max(1)[1].cpu().numpy()[:,:,:]\n","            image = images[0,:,:]        \n","            save_result_comparison(batch['X'], image)\n","                            \n","        output = output.data.cpu().numpy()\n","\n","        N, _, h, w = output.shape                \n","        pred = output.transpose(0, 2, 3, 1).reshape(-1, num_class).argmax(axis=1).reshape(N, h, w)        \n","        target = batch['l'].cpu().numpy().reshape(N, h, w)\n","\n","        for p, t in zip(pred, target):\n","            total_ious.append(iou(p, t))\n","            pixel_accs.append(pixel_acc(p, t))\n","\n","    # Calculate average IoU\n","    total_ious = np.array(total_ious).T  # n_class * val_len\n","    ious = np.nanmean(total_ious, axis=1)\n","    pixel_accs = np.array(pixel_accs).mean()\n","    print(\"epoch{}, pix_acc: {}, meanIoU: {}, IoUs: {}\".format(epoch, pixel_accs, np.nanmean(ious), ious))\n","    \n","    global pixel_acc_list\n","    global mIOU_list\n","    \n","    pixel_acc_list.append(pixel_accs)\n","    mIOU_list.append(np.nanmean(ious))\n","\n","\n","# borrow functions and modify it from https://github.com/Kaixhin/FCN-semantic-segmentation/blob/master/main.py\n","# Calculates class intersections over unions\n","def iou(pred, target):\n","    ious = []\n","    for cls in range(num_class):\n","        pred_inds = pred == cls\n","        target_inds = target == cls\n","        intersection = pred_inds[target_inds].sum()\n","        union = pred_inds.sum() + target_inds.sum() - intersection\n","        if union == 0:\n","            ious.append(float('nan'))  # if there is no ground truth, do not include in evaluation\n","        else:\n","            ious.append(float(intersection) / max(union, 1))\n","        # print(\"cls\", cls, pred_inds.sum(), target_inds.sum(), intersection, float(intersection) / max(union, 1))\n","    return ious\n","\n","\n","def pixel_acc(pred, target):\n","    correct = (pred == target).sum()\n","    total   = (target == target).sum()\n","    return correct / total\n","\n","\n","## perform training and validation\n","val(0)  # show the accuracy before training\n","train()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["---------iter=0\n","epoch0, pix_acc: 0.02212677001953125, meanIoU: 0.005827112149470559, IoUs: [5.62015006e-06 5.70324979e-05 0.00000000e+00 1.66008981e-03\n"," 3.43114637e-02 4.79476550e-03 0.00000000e+00 0.00000000e+00\n"," 1.74474060e-02 4.73999256e-03 1.08186343e-03]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["epoch0, iter0, loss: 0.6900122761726379\n","epoch0, iter10, loss: 0.6352177262306213\n","epoch0, iter20, loss: 0.46853724122047424\n","epoch0, iter30, loss: 0.3772956132888794\n","epoch0, iter40, loss: 0.30217814445495605\n","epoch0, iter50, loss: 0.26683682203292847\n","epoch0, iter60, loss: 0.24340564012527466\n","Finish epoch 0, time elapsed 31.7631995677948\n","---------iter=0\n","epoch0, pix_acc: 0.37079193115234377, meanIoU: 0.05587348176824933, IoUs: [0.02348648 0.14468562 0.         0.38033951 0.00228226 0.05995873\n"," 0.         0.00142198 0.00072057 0.         0.00171315]\n","epoch1, iter0, loss: 0.23727735877037048\n","epoch1, iter10, loss: 0.2585490942001343\n","epoch1, iter20, loss: 0.21012349426746368\n","epoch1, iter30, loss: 0.21003645658493042\n","epoch1, iter40, loss: 0.19253653287887573\n","epoch1, iter50, loss: 0.1950305998325348\n","epoch1, iter60, loss: 0.15848387777805328\n","Finish epoch 1, time elapsed 31.986791133880615\n","---------iter=0\n","epoch1, pix_acc: 0.5720443725585938, meanIoU: 0.15885015097350186, IoUs: [5.15430885e-01 3.54689549e-01 0.00000000e+00 7.47346780e-01\n"," 3.86427553e-03 3.81746255e-02 0.00000000e+00 6.18107866e-05\n"," 8.75950371e-02 1.29399586e-05 1.75758079e-04]\n","epoch2, iter0, loss: 0.15141178667545319\n","epoch2, iter10, loss: 0.1586686074733734\n","epoch2, iter20, loss: 0.11743568629026413\n","epoch2, iter30, loss: 0.11867046356201172\n","epoch2, iter40, loss: 0.15076632797718048\n","epoch2, iter50, loss: 0.15431581437587738\n","epoch2, iter60, loss: 0.12267576903104782\n","Finish epoch 2, time elapsed 32.12568378448486\n","---------iter=0\n","epoch2, pix_acc: 0.6215690612792969, meanIoU: 0.20236864181470324, IoUs: [7.77503413e-01 4.54924748e-01 0.00000000e+00 8.07212888e-01\n"," 1.43186483e-02 5.79499559e-02 0.00000000e+00 3.36000551e-04\n"," 1.13792512e-01 0.00000000e+00 1.68954589e-05]\n","epoch3, iter0, loss: 0.1165124922990799\n","epoch3, iter10, loss: 0.10674186795949936\n","epoch3, iter20, loss: 0.11587797850370407\n","epoch3, iter30, loss: 0.11116731911897659\n","epoch3, iter40, loss: 0.1121717244386673\n","epoch3, iter50, loss: 0.12989991903305054\n","epoch3, iter60, loss: 0.11303240060806274\n","Finish epoch 3, time elapsed 32.13431143760681\n","---------iter=0\n","epoch3, pix_acc: 0.738919677734375, meanIoU: 0.28697137989818017, IoUs: [8.36755940e-01 6.68150927e-01 0.00000000e+00 7.92701599e-01\n"," 2.09925980e-02 6.83965833e-01 0.00000000e+00 8.33020793e-05\n"," 1.54034981e-01 0.00000000e+00 0.00000000e+00]\n","epoch4, iter0, loss: 0.07426164299249649\n","epoch4, iter10, loss: 0.10887784510850906\n","epoch4, iter20, loss: 0.07663190364837646\n","epoch4, iter30, loss: 0.0990787148475647\n","epoch4, iter40, loss: 0.10613364726305008\n","epoch4, iter50, loss: 0.0869966372847557\n","epoch4, iter60, loss: 0.0693645030260086\n","Finish epoch 4, time elapsed 32.09745955467224\n","---------iter=0\n","epoch4, pix_acc: 0.7787062072753906, meanIoU: 0.3248368047638895, IoUs: [8.94730337e-01 7.17309718e-01 1.76678445e-05 8.05621304e-01\n"," 1.35485835e-01 8.19203403e-01 0.00000000e+00 7.66880847e-05\n"," 2.00759900e-01 0.00000000e+00 0.00000000e+00]\n","epoch5, iter0, loss: 0.08085962384939194\n","epoch5, iter10, loss: 0.07974308729171753\n","epoch5, iter20, loss: 0.07873259484767914\n","epoch5, iter30, loss: 0.07128660380840302\n","epoch5, iter40, loss: 0.0851205363869667\n","epoch5, iter50, loss: 0.07848290354013443\n","epoch5, iter60, loss: 0.08517993986606598\n","Finish epoch 5, time elapsed 32.11337375640869\n","---------iter=0\n","epoch5, pix_acc: 0.8096047973632813, meanIoU: 0.3612752965517927, IoUs: [0.89161695 0.67886497 0.         0.85862316 0.35980189 0.81360432\n"," 0.         0.         0.37151696 0.         0.        ]\n","epoch6, iter0, loss: 0.07089707255363464\n","epoch6, iter10, loss: 0.09699302911758423\n","epoch6, iter20, loss: 0.06575095653533936\n","epoch6, iter30, loss: 0.08544512838125229\n","epoch6, iter40, loss: 0.07590892165899277\n","epoch6, iter50, loss: 0.055405475199222565\n","epoch6, iter60, loss: 0.0734235867857933\n","Finish epoch 6, time elapsed 32.26987648010254\n","---------iter=0\n","epoch6, pix_acc: 0.8174678039550781, meanIoU: 0.3734360144917405, IoUs: [8.98535026e-01 7.30844906e-01 1.26027831e-03 8.88910841e-01\n"," 5.39055282e-01 8.06228357e-01 3.80533743e-05 1.44950829e-03\n"," 2.41424280e-01 4.96275130e-05 0.00000000e+00]\n","epoch7, iter0, loss: 0.0652872771024704\n","epoch7, iter10, loss: 0.0754920095205307\n","epoch7, iter20, loss: 0.07839658111333847\n","epoch7, iter30, loss: 0.07805344462394714\n","epoch7, iter40, loss: 0.0907004177570343\n","epoch7, iter50, loss: 0.06364062428474426\n","epoch7, iter60, loss: 0.07231456786394119\n","Finish epoch 7, time elapsed 32.21664214134216\n","---------iter=0\n","epoch7, pix_acc: 0.8322874450683594, meanIoU: 0.3903398601951247, IoUs: [9.05053435e-01 7.54910267e-01 2.11936826e-03 9.13979544e-01\n"," 6.55510271e-01 7.85446321e-01 0.00000000e+00 2.24830829e-04\n"," 2.76279385e-01 2.15039134e-04 0.00000000e+00]\n","epoch8, iter0, loss: 0.05816055089235306\n","epoch8, iter10, loss: 0.059995729476213455\n","epoch8, iter20, loss: 0.06760041415691376\n","epoch8, iter30, loss: 0.08198656141757965\n","epoch8, iter40, loss: 0.05715420842170715\n","epoch8, iter50, loss: 0.0771818608045578\n","epoch8, iter60, loss: 0.06807585805654526\n","Finish epoch 8, time elapsed 32.13165879249573\n","---------iter=0\n","epoch8, pix_acc: 0.8276678466796875, meanIoU: 0.38565772236712376, IoUs: [9.00327899e-01 7.44769318e-01 9.31477791e-04 8.98565047e-01\n"," 5.85065607e-01 7.78437777e-01 0.00000000e+00 2.26115998e-03\n"," 3.31670872e-01 2.02046359e-04 3.74266905e-06]\n","epoch9, iter0, loss: 0.07449929416179657\n","epoch9, iter10, loss: 0.05216996744275093\n","epoch9, iter20, loss: 0.06145119667053223\n","epoch9, iter30, loss: 0.05376704782247543\n","epoch9, iter40, loss: 0.07496154308319092\n","epoch9, iter50, loss: 0.05834902450442314\n","epoch9, iter60, loss: 0.06694480031728745\n","Finish epoch 9, time elapsed 32.10930800437927\n","---------iter=0\n","epoch9, pix_acc: 0.8346171569824219, meanIoU: 0.3918266747324294, IoUs: [8.97769188e-01 7.24645583e-01 3.78932506e-03 9.10585670e-01\n"," 6.16817400e-01 7.93547933e-01 3.64187567e-04 6.38807611e-03\n"," 3.55949875e-01 2.36184692e-04 0.00000000e+00]\n","epoch10, iter0, loss: 0.059616632759571075\n","epoch10, iter10, loss: 0.06279401481151581\n","epoch10, iter20, loss: 0.07295510172843933\n","epoch10, iter30, loss: 0.07041704654693604\n","epoch10, iter40, loss: 0.07466121017932892\n","epoch10, iter50, loss: 0.05571429058909416\n","epoch10, iter60, loss: 0.0527525395154953\n","Finish epoch 10, time elapsed 32.083322048187256\n","---------iter=0\n","epoch10, pix_acc: 0.8362762451171875, meanIoU: 0.3993736354544909, IoUs: [9.03603299e-01 7.17846974e-01 2.10060080e-03 9.15680322e-01\n"," 6.19558614e-01 7.86920702e-01 1.31501170e-04 1.30850872e-02\n"," 4.34179617e-01 0.00000000e+00 3.27326885e-06]\n","epoch11, iter0, loss: 0.05102970451116562\n","epoch11, iter10, loss: 0.06067672744393349\n","epoch11, iter20, loss: 0.08196385949850082\n","epoch11, iter30, loss: 0.06666744500398636\n","epoch11, iter40, loss: 0.07021374255418777\n","epoch11, iter50, loss: 0.06609503924846649\n","epoch11, iter60, loss: 0.05867661163210869\n","Finish epoch 11, time elapsed 32.117000579833984\n","---------iter=0\n","epoch11, pix_acc: 0.8420240783691406, meanIoU: 0.40541718574048874, IoUs: [9.10107841e-01 7.51595193e-01 2.72728176e-03 9.28854659e-01\n"," 7.20615448e-01 7.68818913e-01 3.32743996e-04 1.47103810e-02\n"," 3.61798839e-01 0.00000000e+00 2.77432161e-05]\n","epoch12, iter0, loss: 0.05083116516470909\n","epoch12, iter10, loss: 0.059276971966028214\n","epoch12, iter20, loss: 0.06656114012002945\n","epoch12, iter30, loss: 0.07029053568840027\n","epoch12, iter40, loss: 0.05265181511640549\n","epoch12, iter50, loss: 0.0692821517586708\n","epoch12, iter60, loss: 0.062100961804389954\n","Finish epoch 12, time elapsed 32.14391303062439\n","---------iter=0\n","epoch12, pix_acc: 0.8432110595703125, meanIoU: 0.40767117818055804, IoUs: [9.05473043e-01 7.19202430e-01 2.67790247e-03 9.31317841e-01\n"," 6.94633501e-01 8.18907318e-01 2.51787056e-03 3.31416745e-02\n"," 3.75975410e-01 5.27998414e-04 7.97043284e-06]\n","epoch13, iter0, loss: 0.05566449463367462\n","epoch13, iter10, loss: 0.05564192682504654\n","epoch13, iter20, loss: 0.048718661069869995\n","epoch13, iter30, loss: 0.05147939920425415\n","epoch13, iter40, loss: 0.04232846200466156\n","epoch13, iter50, loss: 0.047094911336898804\n","epoch13, iter60, loss: 0.045120783150196075\n","Finish epoch 13, time elapsed 32.01325082778931\n","---------iter=0\n","epoch13, pix_acc: 0.8347015380859375, meanIoU: 0.39906876832492233, IoUs: [0.90101157 0.75601727 0.01091597 0.93456353 0.69788783 0.76531678\n"," 0.00637636 0.04051289 0.26826165 0.0088926  0.        ]\n","epoch14, iter0, loss: 0.05414290726184845\n","epoch14, iter10, loss: 0.04250287264585495\n","epoch14, iter20, loss: 0.057609472423791885\n","epoch14, iter30, loss: 0.07306680083274841\n","epoch14, iter40, loss: 0.06238473579287529\n","epoch14, iter50, loss: 0.057847242802381516\n","epoch14, iter60, loss: 0.04481111839413643\n","Finish epoch 14, time elapsed 32.10660648345947\n","---------iter=0\n","epoch14, pix_acc: 0.8421920776367188, meanIoU: 0.4159314863386944, IoUs: [9.09346162e-01 7.82845168e-01 1.11815017e-02 9.28704455e-01\n"," 7.06180060e-01 7.99334914e-01 2.41217383e-02 1.16722995e-01\n"," 2.92678139e-01 3.96718190e-03 1.64035943e-04]\n","epoch15, iter0, loss: 0.07225720584392548\n","epoch15, iter10, loss: 0.0674714520573616\n","epoch15, iter20, loss: 0.05161119997501373\n","epoch15, iter30, loss: 0.05416816473007202\n","epoch15, iter40, loss: 0.05997712165117264\n","epoch15, iter50, loss: 0.04312644153833389\n","epoch15, iter60, loss: 0.04679526016116142\n","Finish epoch 15, time elapsed 32.107410192489624\n","---------iter=0\n","epoch15, pix_acc: 0.8448495483398437, meanIoU: 0.41252681367870375, IoUs: [9.04914196e-01 7.70458473e-01 1.36795049e-02 9.37323935e-01\n"," 6.96781502e-01 7.87214788e-01 2.15461516e-02 7.80235442e-02\n"," 3.19172785e-01 8.57909093e-03 1.00977719e-04]\n","epoch16, iter0, loss: 0.05136977136135101\n","epoch16, iter10, loss: 0.055179353803396225\n","epoch16, iter20, loss: 0.06680239737033844\n","epoch16, iter30, loss: 0.06593779474496841\n","epoch16, iter40, loss: 0.05167687311768532\n","epoch16, iter50, loss: 0.04656011611223221\n","epoch16, iter60, loss: 0.056439097970724106\n","Finish epoch 16, time elapsed 31.98685574531555\n","---------iter=0\n","epoch16, pix_acc: 0.8498980712890625, meanIoU: 0.432800189918725, IoUs: [9.09346489e-01 7.79122553e-01 1.11558182e-02 9.30031105e-01\n"," 6.99902966e-01 8.14635163e-01 8.81898723e-02 1.80265266e-01\n"," 3.14755713e-01 3.32366185e-02 1.60525080e-04]\n","epoch17, iter0, loss: 0.042214516550302505\n","epoch17, iter10, loss: 0.05303323641419411\n","epoch17, iter20, loss: 0.040032386779785156\n","epoch17, iter30, loss: 0.043447840958833694\n","epoch17, iter40, loss: 0.05434997379779816\n","epoch17, iter50, loss: 0.05579817667603493\n","epoch17, iter60, loss: 0.034521423280239105\n","Finish epoch 17, time elapsed 32.05821752548218\n","---------iter=0\n","epoch17, pix_acc: 0.85492431640625, meanIoU: 0.4449821671307948, IoUs: [9.06194730e-01 7.59520477e-01 1.47722606e-02 9.21205085e-01\n"," 7.17322573e-01 8.43861855e-01 1.07664761e-01 2.55113653e-01\n"," 3.33716976e-01 3.45195909e-02 9.11877616e-04]\n","epoch18, iter0, loss: 0.04763804003596306\n","epoch18, iter10, loss: 0.051079992204904556\n","epoch18, iter20, loss: 0.055282287299633026\n","epoch18, iter30, loss: 0.06507422775030136\n","epoch18, iter40, loss: 0.05498430132865906\n","epoch18, iter50, loss: 0.049291450530290604\n","epoch18, iter60, loss: 0.040944114327430725\n","Finish epoch 18, time elapsed 32.17915987968445\n","---------iter=0\n","epoch18, pix_acc: 0.8605741882324218, meanIoU: 0.4559691928080364, IoUs: [0.90874852 0.78221802 0.00353808 0.93315226 0.70800036 0.83415917\n"," 0.10002002 0.32887537 0.38196846 0.03277587 0.00220498]\n","epoch19, iter0, loss: 0.04761800915002823\n","epoch19, iter10, loss: 0.051184967160224915\n","epoch19, iter20, loss: 0.04184671863913536\n","epoch19, iter30, loss: 0.04598069190979004\n","epoch19, iter40, loss: 0.041524361819028854\n","epoch19, iter50, loss: 0.05275597795844078\n","epoch19, iter60, loss: 0.036155395209789276\n","Finish epoch 19, time elapsed 32.00953650474548\n","---------iter=0\n","epoch19, pix_acc: 0.8586553955078124, meanIoU: 0.46098138627104573, IoUs: [0.89227637 0.74585694 0.00169864 0.93936478 0.70110332 0.83506325\n"," 0.07306111 0.37633529 0.44536842 0.0512591  0.00940804]\n","The highest mIOU is 0.46098138627104573 and is achieved at epoch-20\n","The highest pixel accuracy  is 0.8605741882324218 and is achieved at epoch-19\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GA8VUYdHstYy","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}